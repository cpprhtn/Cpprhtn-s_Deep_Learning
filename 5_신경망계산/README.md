# 신경망 계산
이번에는 직접, 신경망이 어떻게 계산이 되는지 확인해 볼 것이다.

## 상황 가정
여러분이 고양이 한마리를 키운다고 가정한다.
![5-2](https://user-images.githubusercontent.com/63298243/92117574-4dd70700-ee30-11ea-91e4-58b433ea913f.jpg)

고양이의 행동패턴을 보며 살이 얼만큼 찌는지 확인해보고 간식량을 조절하는게 목표이다.

여러분이 고양이를 관찰하면서 아래 표를 기록했다.
<고양이 관찰 일지>
|Index|간식 섭취 횟수|놀이시간|수면시간|몸무게 증가량|
|:---:|:---|---|---|---|
|0|4|3|14|1%|
|1|2|2|16|1.4%|
|2|2|4|12|0.3%|
|.|.|.|.|.|
|.|.|.|.|.|
* 고양이 평균 수면시간: 12 ~ 16시간

## 신경망 계획
- 목표: 몸무게 증가량을 보고 간식량 조절

- 입력층: 3개 (간식 섭취 횟수, 놀이시간, 수면시간)
- 출력층: 1개 (몸무게 증가량)
  - 선형/회귀중에 회귀
- 은닉층: 임의로 설정
- 활성화 함수: ReLU (softmax을 사용해도 상관없다)
- 손실함수: 생략 (이후에 배울 예정)

## 신경망 구현
<3층신경망 구상>

<img width="1320" alt="5-1" src="https://user-images.githubusercontent.com/63298243/92112969-90491580-ee29-11ea-8db7-5ec3a7fbb696.png">



<변수 설명>
- 입력층
  - x1: 간식 섭취 횟수
  - x2: 놀이시간
  - x3: 수면시간
- 은닉층 1: z1, z2
- 은닉층 2: z1, z2
- 출력층 y: 몸무게 증가량

<img width="1320" alt="5-3" src="https://user-images.githubusercontent.com/63298243/92117580-4fa0ca80-ee30-11ea-9c21-b8e902e9c625.png">


<W(가중치),b(편향) 설정>
* W(ab) : 화살표가 도착하는 방향의 변수 번호(a) + 화살표가 출발하는 방향의 변수 번호(b)
  - 정공법: 0.01 * np.random.randn(1,100)처럼 표준편차가 0.01인 정규분포를 사용하는것 (0.01 * 정규분포에서 생성된 값)
  - 현재: 계산의 편의성을 위해 0.1~0.5사이의 값을 무작위로 사용
* b(bias) : 주로 1을 사용하지만 여기서는 0.2를 사용해볼 것.

- Input layer -> hidden layer 1
  - W11: 0.2
  - W12: 0.1
  - W13: 0.4
  - W21: 0.5
  - W22: 0.3
  - W23: 0.1

- hidden layer 1 -> hidden layer 2
  - W11: 0.2
  - W12: 0.4
  - W21: 0.3
  - W22: 0.2

- hidden layer 2 -> Output layer
  - W11: 0.1
  - W12: 0.2

- b(편향): 0.2

<img width="1320" alt="5-4" src="https://user-images.githubusercontent.com/63298243/92117584-52032480-ee30-11ea-8b03-417ddf35dfd0.png">

<계산>
**고양이 관찰 일지 표의 index 0번의 값을 사용**
- hidden layer 1
  - z1 = (x1 * W11) + (x2 * W12) + (x3 * W13) + b = (4 * 0.2) + (3 * 0.1) + (5 * 0.4) + 0.2 = 3.3
  - z2 = (x1 * W21) + (x2 * W22) + (x3 * W23) + b = (4 * 0.5) + (3 * 0.3) + (5 * 0.1) + 0.2 = 3.6

- hidden layer 2
  - z1 = (x1[hidden layer 1의 z1] * W11) + (x2 * W12) + b = (3.3 * 0.2) + (3.6 * 0.3) + 0.2 = 1.94
  - z2 = (x1 * W21) + (x2 * W22) + b = (3.3 * 0.4) + (3.6 * 0.1) + 0.2 = 1.88

- Output
  - y = (x1[hidden layer 2의 z1] * W11) + (x2 * W12) + b = (1.94 * 0.1) + (1.88 * 0.2) + 0.2 = 0.77

- ReLU 함수
<img width="188" alt="2-4" src="https://user-images.githubusercontent.com/63298243/92236221-fc8e4c80-eeef-11ea-8e14-958026fbef98.png">
**위 계산 과정에서 음수가 없으므로 그대로 출력**

## 결과 확인
- 고양이 관찰일지의 index 0번 값을 사용했다.
- 실제 관찰값한 몸무게 증가량은 1%였으나, 임의의 가중치를 적용한 몸무게 증가량은 0.77%가 나왔다.

- 결과적인 오차 값은 (1 - 0.77 = 0.23)% 이다.
- 이를 W(가중치)와 b(편향) 값을 조절해 가며 1%에 근사하게 학습시킬 수 있다.

### 추가적인 말
현재까지 구현 해본 신경망은 입력층에서 출력층까지 가는 Feedforward Neural Network(순방향 신경망)이다.

임의로 W와 b의 값을 수정해 나가며 최적의 값을 찾을 수도 있지만,

앞으로 배울 손실함수와 backward NN(역방향 신경망)을 통해서 알맞은 값들을 찾아낼 수도 있다.
