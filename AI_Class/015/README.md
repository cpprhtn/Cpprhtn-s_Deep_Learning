# 신경망 연산

- **n층의 노드 하나에서 n+1층의 모든 노드로 화살표가 뻗어나가며, 각 화살표마다 가중치(W)가 필요하다.**

- 신경망은 행렬의 곱으로 연산을 수행한다.
- numpy.dot(a, b) 형태로 a와 b의 행렬을 연산한다.

![3-1](https://user-images.githubusercontent.com/63298243/90893755-81f70480-e3fa-11ea-9515-216b132b0495.png)

## 신경망의 연산 규칙

![3-3](https://user-images.githubusercontent.com/63298243/90893760-84f1f500-e3fa-11ea-98db-4fce7d2ec204.png)


- 위의 신경망은 n층 신경망이라 가정하자.
  - l=1 (0층 -> 1층)
    - 0층의 i, i+1이 입력 신호이며, j-1, j, j+1이 출력 신호가 된다.
  - l=2 (1층 -> 2층)
    - 1층의 j-1, j, j+1이 입력신호가 되며, 2층의 k, k+1, ... , k+n이 출력 신호가 된다.

  - **즉 l층이 입력층이 될 수도, 출력층이 될 수도 있다. 단 처음층은 입력층만, 마지막층은 출력층만 된다.**

- 위 그림의 파란 뉴런(노드)처럼 'l-1층 -> l층' 형태로 연산시에 항상 편향값(b)이 존재한다.
  - 위 그림의 연산 후, 'l층 -> l+1' 형태로 연산시, l층 위에 편향값이 생긴다.

  - **즉 l층의 편향값에서도 l+1층의 모든 뉴런에 화살표가 뻗어나간다.

## 일반식 H(x)
![3-2](https://user-images.githubusercontent.com/63298243/90893771-88857c00-e3fa-11ea-9800-755fb73f2de0.png)

- 입력신호에서 출력신호로 도달할때, 가중치(W)와 편향(b)을 적용한 일반식을 h(x)라고 한다.


## 기본적인 신경망 연산 과정

1. 입력층에서 가중치(W)와 편향(b)을 통해 출력층에 연산값이 도달한다. 이때 도달한 값을 a라고 부른다.

2. 활성화 함수(시그모이드, 렐루 등)를 적용한다. 이때, 활성화 함수가 적용. 값을 z라고 부른다.

3. 활성화 함수를 적용시킨 출력 신호는 다시 다음층의 입력신호가 되며, 이는 z라고 부른다.

0층 -> 1층:

x -> a -> z

1층 -> 2층:

z -> a -> z

2층 -> 3층:

z -> a -> z

.

.

.

-> 마지막 층:

z -> a -> y
