# 활성화 함수2
앞에서 배운 시그모이드(sigmoid), 렐루(ReLU) 등의 활성화함수는 각 층의 뉴런(노드)에서 뉴런으로 값이 전달 할 때, 다음층에 전달될 값을 조절할 때 사용되었다.

이번에 배울 활성화 함수는 **출력층 활성화 함수**라고 부르며, 크게 항등함수와 소프트맥(softmax) 함수가 있다.
